{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import jieba  \n",
    "from config import MyConfig, MyError\n",
    "    \n",
    "class DataUtility(object):    \n",
    "    def __init__(self):\n",
    "        self.gConfig = MyConfig.get_config(config_file=\"./config/seq2seq.ini\") \n",
    "        self.conv_path = self.gConfig[\"resource_data\"]\n",
    "        self.sep_token = self.gConfig[\"sep_token\"]\n",
    "        if not os.path.exists(self.conv_path):\n",
    "            raise MyError(\"檔案不存在\")\n",
    "            exit()\n",
    "        self.dir_path = self.gConfig[\"train_data\"]\n",
    "        self.file_path = self.gConfig[\"seq_data\"]        \n",
    "        os.makedirs(self.dir_path, exist_ok=True)\n",
    "        if os.path.isfile(self.file_path):\n",
    "            print(self.file_path, \"Exist\")\n",
    "        else:  \n",
    "            self.preprocess_train_data()\n",
    "            \n",
    "    def get_conv_data(self):        \n",
    "        convs = []\n",
    "        M = self.gConfig[\"m\"]\n",
    "        E = self.gConfig[\"e\"]\n",
    "        with open(self.conv_path, encoding=\"utf8\") as f:\n",
    "            one_conv = []\n",
    "            for line in f:\n",
    "                line = line.strip(\"\\n\").replace(\"/\",\"\")\n",
    "                if line == \"\":\n",
    "                    continue\n",
    "                if line[0] == M: #是問答，放入問答\n",
    "                    one_conv.append(line.split(\" \")[1])\n",
    "                elif line[0] == E: #是空白\n",
    "                    if one_conv: #問答裡面有資料\n",
    "                        convs.append(one_conv)\n",
    "                    one_conv = []\n",
    "        return convs\n",
    "    \n",
    "    def tokenize_convs(self, convs):\n",
    "        seq = []\n",
    "        for conv in convs:\n",
    "            if len(conv) == 1:\n",
    "                continue\n",
    "            if len(conv)%2 != 0:\n",
    "                conv = conv[:-1]\n",
    "            for i in range(len(conv)):\n",
    "                if i%2 ==0:\n",
    "                    conv[i] = \" \".join(jieba.cut(conv[i]))\n",
    "                    conv[i+1] = \" \".join(jieba.cut(conv[i+1]))\n",
    "                    seq.append(conv[i]+self.sep_token+conv[i+1])\n",
    "        return seq\n",
    "    def save_tokenize_data(self, seq):        \n",
    "        content = \"\"\n",
    "        for i in range(len(seq)):\n",
    "            content += seq[i] + \"\\n\"\n",
    "            if i%1000 == 0:\n",
    "                print(\".\",end=\"\")\n",
    "        with open(self.file_path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(content)\n",
    "            print(\"finish preprocessing seq data\")\n",
    "        \n",
    "    def preprocess_train_data(self):        \n",
    "        convs = self.get_conv_data()\n",
    "        seq = self.tokenize_convs(convs)\n",
    "        self.save_tokenize_data(seq)\n",
    "        return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/seq.data Exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DataUtility at 0x242c9e9a048>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataUtility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "from config import MyConfig, MyError\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gConfig = MyConfig.get_config(\"./config/seq2seq.ini\")\n",
    "        self.enc_vocab_size = self.gConfig[\"enc_vocab_size\"]\n",
    "\n",
    "        self.embedding_dim = self.gConfig[\"embedding_dim\"]\n",
    "        self.layer_size = self.gConfig[\"layer_size\"]\n",
    "        self.batch_size = self.gConfig[\"batch_size\"]\n",
    "        self.embedding_layer = Embedding(self.enc_vocab_size, self.embedding_dim)\n",
    "        self.GRU = GRU(self.layer_size, return_sequences=True, \n",
    "                       return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding_layer(x)\n",
    "        output, state = self.GRU(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.layer_size))\n",
    "\n",
    "class BahdanauAttention(Model): #定義 Attention 機制\n",
    "    def __init__(self, layer_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(layer_size)\n",
    "        self.W2 = Dense(layer_size)\n",
    "        self.V = Dense(1) #最後的評分網路層V，最終評分結果作為注意力的權重值\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        # 計算完 W1, W2，將結果輸入評分網路層\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1) #得到各個score值的機率分布\n",
    "         # 文字向量 context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values # 得到加權後的文字向量\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights #返回加權後的文字向量 和 注意力權重\n",
    "        \n",
    "    \n",
    "class Decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.gConfig = MyConfig.get_config(\"./config/seq2seq.ini\")\n",
    "        self.dec_vocab_size = self.gConfig[\"dec_vocab_size\"] \n",
    "        self.layer_size = self.gConfig[\"layer_size\"]\n",
    "        self.embedding_dim = self.gConfig[\"embedding_dim\"]\n",
    "        self.embedding_layer = Embedding(self.dec_vocab_size, self.embedding_dim)\n",
    "        self.GRU = GRU(self.layer_size, return_sequences=True,\n",
    "                       return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        self.fc = Dense(self.dec_vocab_size)\n",
    "        self.attention = BahdanauAttention(self.layer_size)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding_layer(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.GRU(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        outputs = self.fc(output)\n",
    "        return outputs, state, attention_weights\n",
    "    \n",
    "gConfig = MyConfig.get_config(\"./config/seq2seq.ini\")\n",
    "\n",
    "def loss_function(real, pred):    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def train_step(inp, targ, targ_lang, enc_hidden):    \n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['start']] * gConfig[\"batch_size\"], 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:,t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:,t], 1)\n",
    "    batch_loss = (loss/int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + encoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss      \n",
    "\n",
    "encoder = Encoder()\n",
    "attention_layer = BahdanauAttention(10)\n",
    "decoder = Decoder()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)    \n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, io, time\n",
    "from config import MyConfig, MyError\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data in train_data\n",
      "batch_loss 0 0.49792975\n",
      "batch_loss 1 0.5232644\n",
      "batch_loss 2 0.45578578\n",
      "batch_loss 3 0.4307389\n",
      "batch_loss 4 0.43929917\n",
      "batch_loss 5 0.4689792\n",
      "訓練總步數: 6 每步耗時: 36.87430588404337  最新每步耗時: 36.87414010365804 最新每步loss 0.4693\n",
      "batch_loss 0 0.56297344\n",
      "batch_loss 1 0.4261917\n",
      "batch_loss 2 0.41870934\n",
      "batch_loss 3 0.49716806\n",
      "batch_loss 4 0.45919925\n",
      "batch_loss 5 0.48236874\n",
      "訓練總步數: 12 每步耗時: 43.73586151997248  最新每步耗時: 50.59741715590159 最新每步loss 0.4744\n",
      "batch_loss 0 0.48051623\n",
      "batch_loss 1 0.46108463\n",
      "batch_loss 2 0.51002973\n",
      "batch_loss 3 0.44710365\n",
      "batch_loss 4 0.49057743\n",
      "batch_loss 5 0.46804363\n",
      "訓練總步數: 18 每步耗時: 49.28294071886275  最新每步耗時: 60.37693258126577 最新每步loss 0.4762\n",
      "checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = start_token+\" \"+w+\" \"+end_token\n",
    "    return w\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='utf-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = Tokenizer(num_words=vocab_inp_size, oov_token=3)\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    input_lang, target_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, input_tokenizer = tokenize(input_lang)\n",
    "    target_tensor, target_tokenizer = tokenize(target_lang)\n",
    "    return input_tensor, input_tokenizer, target_tensor, target_tokenizer\n",
    "\n",
    "\n",
    "gConfig = MyConfig.get_config(\"./config/seq2seq.ini\")\n",
    "file_path = gConfig[\"seq_data\"]\n",
    "num_examples = gConfig[\"max_train_data_size\"]\n",
    "start_token = gConfig[\"start_token\"]\n",
    "end_token = gConfig[\"end_token\"]\n",
    "sep_token = gConfig[\"sep_token\"]\n",
    "vocab_inp_size = gConfig[\"enc_vocab_size\"]\n",
    "vocab_tar_size = gConfig[\"dec_vocab_size\"]\n",
    "embedding_dim = gConfig[\"embedding_dim\"]\n",
    "layer_size = gConfig[\"layer_size\"]\n",
    "batch_size = gConfig[\"batch_size\"]\n",
    "checkpoint_dir = gConfig[\"model_data\"]\n",
    "epochs = gConfig[\"epochs\"]\n",
    "# global epoch_idnex\n",
    "# epoch_idnex = 0 \n",
    "\n",
    "\n",
    "    \n",
    "input_tensor,input_tokenizer,target_tensor,target_tokenizer = load_dataset(file_path, num_examples)\n",
    "max_length_inp,max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "#input_tensor,input_token,target_tensor,target_token= read_data(gConfig['seq_data'], gConfig['max_train_data_size'])\n",
    "\n",
    "def train():\n",
    "    epoch_index = 0\n",
    "    print(\"Preparing data in %s\" % gConfig['train_data'])\n",
    "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
    "                                                                                target_tensor, test_size=0.2)\n",
    "    steps_per_epoch = len(input_tensor_train) // batch_size\n",
    "    BUFFER_SIZE = len(input_tensor_train)    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    #\n",
    "#     ckpt = tf.io.gfile.listdir(checkpoint_dir)\n",
    "#     if ckpt:\n",
    "#         print(\"reload pretrained model\")\n",
    "#         checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    \n",
    "    start_time = time.time()    \n",
    "\n",
    "    current_steps = 0\n",
    "    while epoch_index < epochs:\n",
    "    #while True:\n",
    "        start_time_epoch = time.time()\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "        for (batch, (inp,targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, target_tokenizer, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            print(\"batch_loss\", batch, batch_loss.numpy())\n",
    "        step_time_epoch = (time.time() - start_time_epoch) / steps_per_epoch\n",
    "        step_loss = total_loss / steps_per_epoch\n",
    "        current_steps += steps_per_epoch\n",
    "        step_time_total = (time.time()-start_time)/current_steps \n",
    "        print('訓練總步數: {} 每步耗時: {}  最新每步耗時: {} 最新每步loss {:.4f}'\\\n",
    "              .format(current_steps, step_time_total, step_time_epoch, step_loss.numpy()))        \n",
    "        epoch_index += 1\n",
    "\n",
    "train() \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "print(\"checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model():\n",
    "    model = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    return model\n",
    "\n",
    "\n",
    "import jieba  \n",
    "import numpy as np\n",
    "def predict(sentence):\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "#     print(\"sentence\",sentence)\n",
    "#     input_words = []\n",
    "#     input_words.append(start_token)\n",
    "#     jieba_dicts = jieba.cut(sentence)\n",
    "#     for word in jieba_dicts:\n",
    "#         input_words.append(word)\n",
    "#     input_words.append(end_token)\n",
    "#     print(\"input_words\",input_words)\n",
    "    inputs = [input_tokenizer.word_index.get(i,3) for i in sentence.split(\" \")]\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')\n",
    "   \n",
    "    inputs = tf.convert_to_tensor(inputs)  \n",
    "    print(\"inputs\",inputs) \n",
    "    result = \"\"\n",
    "    hidden = [tf.zeros((1, layer_size))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index[start_token]], 0)\n",
    "    for t in range(max_length_tar):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()        \n",
    "       # print(\"predicted_id\",predicted_id)\n",
    "        ran = np.random.randint(low=10, high=500)\n",
    "#         if target_tokenizer.index_word[predicted_id] == end_token:\n",
    "#             break\n",
    "        result += target_tokenizer.index_word[predicted_id + ran] + \" \"\n",
    "        #print(\"pre\",target_tokenizer.index_word[predicted_id + ran])\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    print(\"finished predict\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs tf.Tensor([[ 2  3  3 71  3  3  3  3 17  3  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(1, 22), dtype=int32)\n",
      "finished predict\n",
      "我要 这样 多少 一切 喵 老实 玩 美丽 叫 多 嗯 针 真心 味儿 应该 猪 倒霉 不想 π 比 那么 教 玩 小样 晚安 看不清 以为 喵 呵 要是 宇宙 我爱你 这个 一样 买不起 粉 一些 办 ' 那个 讨厌 毛 招魂 代表 把 ╰ 5 没 关注 比较 热   得 声音 呢 王子 ╮ 吗 粉 隔壁 走 耶 以为 然后 o 饭 用力 难不倒 咬 吃 帮 没 多 v 针 ╭ 看不清 的哥 注意 稀罕 一些 不无 腩 善良 老娘 在 我错 啥 宇宙 治 真 … 认识 关注 起来 tm 怪 没错 边 不理 声音 难不倒 全世界 改 按 了 发现 里 直接 啦 嘛 人家 想 善良 起来 还是 兄弟 赤峰 买 回来 高富帅 ～ 本来 当 爸爸 听 有人 后妈 你好 点饭 废话 一个 滚 性感 远 中 工作 强 就是 饭 \n"
     ]
    }
   ],
   "source": [
    "req_msg = \"第一个反应就是检查门窗是否关好\"\n",
    "req_msg = \" \".join(jieba.cut(req_msg))\n",
    "res_msg = predict(req_msg)\n",
    "print(res_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"第一个反应就是检查门窗是否关好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = jieba.cut(\"第一个反应就是检查门窗是否关好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
