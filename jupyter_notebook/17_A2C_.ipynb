{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, losses, optimizers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProbabilityDistribution(models.Model):\n",
    "    def call(self, logits):\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "class MyModel(models.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(MyModel, self).__init__(name=\"mlp_policy\")\n",
    "        self.hidden1 = layers.Dense(units=128, activation=\"relu\", name=\"hd_1\")\n",
    "        self.hidden2 = layers.Dense(units=128, activation=\"relu\", name=\"hd_2\")\n",
    "        self.value = layers.Dense(units=1, name=\"value\")\n",
    "        self.logits = layers.Dense(num_actions, name=\"policy_logits\")\n",
    "        self.dist = ProbabilityDistribution()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to Tensor\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        # separate hidden layers from the same input tensor\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "    \n",
    "    def action_value(self, obs):\n",
    "        # executes call() under the hood\n",
    "        logits, value = self.predict(obs)\n",
    "        action = self.dist.predict(logits)\n",
    "        # a simpler option, will become clear later why we don't use it\n",
    "        # action = tf.random.categorical(logits, 1)\n",
    "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\n",
    "        \n",
    "class A2CAgent:\n",
    "    def __init__(self, model):\n",
    "        self.params = {\n",
    "            \"gamma\": 0.99,\n",
    "            \"value\": 0.5,\n",
    "            \"entropy\": 0.0001\n",
    "        }\n",
    "        self.model = model\n",
    "        self.model.compile(optimizer=optimizers.RMSprop(lr=0.0007),\n",
    "                           loss=[self._logits_loss, self._value_loss])\n",
    "    \n",
    "    def train(self, env, batch_sz=32, updates=1000):\n",
    "        # storage helpers for a single batch of data\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "        # training loop: collect samples, send to optimizer, repeat updates times\n",
    "        ep_rews = [0.0]\n",
    "        next_obs = env.reset()\n",
    "        for update in range(updates):\n",
    "            for step in range(batch_sz):\n",
    "                observations[step] = next_obs.copy()\n",
    "                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "                ep_rews[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rews.append(0.0)\n",
    "                    next_obs = env.reset()\n",
    "                    logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rews)-1, ep_rews[-2]))\n",
    "\n",
    "            _, next_value = self.model.action_value(next_obs[None, :])\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "            # a trick to input actions and advantages through same API\n",
    "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "            # performs a full training step on the collected batch\n",
    "            # note: no need to mess around with gradients, Keras API handles it\n",
    "            step_losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, step_losses))\n",
    "        return ep_rews\n",
    "        \n",
    "    def test(self, env, render=False):\n",
    "        obs, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(obs[None, :])\n",
    "            obs, reward, doen, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "        return ep_reward        \n",
    "    \n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "    \n",
    "    def _value_loss(self, returns, value):\n",
    "        # value loss is typically MSE between value estimates and returns\n",
    "        return self.params['value']* losses.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, acts_and_advs, logits):\n",
    "        # a trick to input actions and advantages through same API\n",
    "        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n",
    "        # sparse categorical CE loss obj that supports sample_weight arg on call()\n",
    "        # from_logits argument ensures transformation into normalized probabilities\n",
    "        weighted_sparse_ce = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # policy loss is defined by policy gradients, weighted by advantages\n",
    "        # note: we only calculate the loss on the actions we've actually taken\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # entropy loss can be calculated via CE over itself\n",
    "        entropy_loss = losses.categorical_crossentropy(logits, logits, from_logits=True)\n",
    "        # here signs are flipped because optimizer minimizes\n",
    "        return policy_loss - self.params['entropy']*entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode: 001, Reward: 010\n",
      "INFO:root:Episode: 002, Reward: 022\n",
      "INFO:root:Episode: 003, Reward: 016\n",
      "INFO:root:Episode: 004, Reward: 015\n",
      "INFO:root:Episode: 005, Reward: 019\n",
      "INFO:root:Episode: 006, Reward: 010\n",
      "INFO:root:Episode: 007, Reward: 024\n",
      "INFO:root:Episode: 008, Reward: 021\n",
      "INFO:root:Episode: 009, Reward: 010\n",
      "INFO:root:Episode: 010, Reward: 010\n",
      "INFO:root:Episode: 011, Reward: 011\n",
      "INFO:root:Episode: 012, Reward: 009\n",
      "INFO:root:Episode: 013, Reward: 017\n",
      "INFO:root:Episode: 014, Reward: 028\n",
      "INFO:root:Episode: 015, Reward: 026\n",
      "INFO:root:Episode: 016, Reward: 018\n",
      "INFO:root:Episode: 017, Reward: 016\n",
      "INFO:root:Episode: 018, Reward: 049\n",
      "INFO:root:Episode: 019, Reward: 018\n",
      "INFO:root:Episode: 020, Reward: 020\n",
      "INFO:root:Episode: 021, Reward: 015\n",
      "INFO:root:Episode: 022, Reward: 015\n",
      "INFO:root:Episode: 023, Reward: 013\n",
      "INFO:root:Episode: 024, Reward: 013\n",
      "INFO:root:Episode: 025, Reward: 036\n",
      "INFO:root:Episode: 026, Reward: 024\n",
      "INFO:root:Episode: 027, Reward: 031\n",
      "INFO:root:Episode: 028, Reward: 018\n",
      "INFO:root:Episode: 029, Reward: 012\n",
      "INFO:root:Episode: 030, Reward: 022\n",
      "INFO:root:Episode: 031, Reward: 018\n",
      "INFO:root:Episode: 032, Reward: 015\n",
      "INFO:root:Episode: 033, Reward: 018\n",
      "INFO:root:Episode: 034, Reward: 014\n",
      "INFO:root:Episode: 035, Reward: 017\n",
      "INFO:root:Episode: 036, Reward: 014\n",
      "INFO:root:Episode: 037, Reward: 041\n",
      "INFO:root:Episode: 038, Reward: 019\n",
      "INFO:root:Episode: 039, Reward: 014\n",
      "INFO:root:Episode: 040, Reward: 011\n",
      "INFO:root:Episode: 041, Reward: 015\n",
      "INFO:root:Episode: 042, Reward: 013\n",
      "INFO:root:Episode: 043, Reward: 042\n",
      "INFO:root:Episode: 044, Reward: 009\n",
      "INFO:root:Episode: 045, Reward: 021\n",
      "INFO:root:Episode: 046, Reward: 021\n",
      "INFO:root:Episode: 047, Reward: 016\n",
      "INFO:root:Episode: 048, Reward: 012\n",
      "INFO:root:Episode: 049, Reward: 021\n",
      "INFO:root:Episode: 050, Reward: 014\n",
      "INFO:root:Episode: 051, Reward: 018\n",
      "INFO:root:Episode: 052, Reward: 018\n",
      "INFO:root:Episode: 053, Reward: 016\n",
      "INFO:root:Episode: 054, Reward: 017\n",
      "INFO:root:Episode: 055, Reward: 022\n",
      "INFO:root:Episode: 056, Reward: 036\n",
      "INFO:root:Episode: 057, Reward: 017\n",
      "INFO:root:Episode: 058, Reward: 018\n",
      "INFO:root:Episode: 059, Reward: 029\n",
      "INFO:root:Episode: 060, Reward: 016\n",
      "INFO:root:Episode: 061, Reward: 011\n",
      "INFO:root:Episode: 062, Reward: 016\n",
      "INFO:root:Episode: 063, Reward: 033\n",
      "INFO:root:Episode: 064, Reward: 034\n",
      "INFO:root:Episode: 065, Reward: 012\n",
      "INFO:root:Episode: 066, Reward: 015\n",
      "INFO:root:Episode: 067, Reward: 011\n",
      "INFO:root:Episode: 068, Reward: 028\n",
      "INFO:root:Episode: 069, Reward: 011\n",
      "INFO:root:Episode: 070, Reward: 033\n",
      "INFO:root:Episode: 071, Reward: 015\n",
      "INFO:root:Episode: 072, Reward: 016\n",
      "INFO:root:Episode: 073, Reward: 017\n",
      "INFO:root:Episode: 074, Reward: 017\n",
      "INFO:root:Episode: 075, Reward: 017\n",
      "INFO:root:Episode: 076, Reward: 017\n",
      "INFO:root:Episode: 077, Reward: 039\n",
      "INFO:root:Episode: 078, Reward: 012\n",
      "INFO:root:Episode: 079, Reward: 012\n",
      "INFO:root:Episode: 080, Reward: 030\n",
      "INFO:root:Episode: 081, Reward: 014\n",
      "INFO:root:Episode: 082, Reward: 046\n",
      "INFO:root:Episode: 083, Reward: 012\n",
      "INFO:root:Episode: 084, Reward: 025\n",
      "INFO:root:Episode: 085, Reward: 031\n",
      "INFO:root:Episode: 086, Reward: 035\n",
      "INFO:root:Episode: 087, Reward: 039\n",
      "INFO:root:Episode: 088, Reward: 013\n",
      "INFO:root:Episode: 089, Reward: 029\n",
      "INFO:root:Episode: 090, Reward: 047\n",
      "INFO:root:Episode: 091, Reward: 036\n",
      "INFO:root:Episode: 092, Reward: 015\n",
      "INFO:root:Episode: 093, Reward: 039\n",
      "INFO:root:Episode: 094, Reward: 036\n",
      "INFO:root:Episode: 095, Reward: 040\n",
      "INFO:root:Episode: 096, Reward: 013\n",
      "INFO:root:Episode: 097, Reward: 019\n",
      "INFO:root:Episode: 098, Reward: 040\n",
      "INFO:root:Episode: 099, Reward: 035\n",
      "INFO:root:Episode: 100, Reward: 027\n",
      "INFO:root:Episode: 101, Reward: 020\n",
      "INFO:root:Episode: 102, Reward: 019\n",
      "INFO:root:Episode: 103, Reward: 014\n",
      "INFO:root:Episode: 104, Reward: 028\n",
      "INFO:root:Episode: 105, Reward: 014\n",
      "INFO:root:Episode: 106, Reward: 033\n",
      "INFO:root:Episode: 107, Reward: 015\n",
      "INFO:root:Episode: 108, Reward: 024\n",
      "INFO:root:Episode: 109, Reward: 143\n",
      "INFO:root:Episode: 110, Reward: 036\n",
      "INFO:root:Episode: 111, Reward: 017\n",
      "INFO:root:Episode: 112, Reward: 053\n",
      "INFO:root:Episode: 113, Reward: 029\n",
      "INFO:root:Episode: 114, Reward: 046\n",
      "INFO:root:Episode: 115, Reward: 022\n",
      "INFO:root:Episode: 116, Reward: 059\n",
      "INFO:root:Episode: 117, Reward: 097\n",
      "INFO:root:Episode: 118, Reward: 015\n",
      "INFO:root:Episode: 119, Reward: 014\n",
      "INFO:root:Episode: 120, Reward: 022\n",
      "INFO:root:Episode: 121, Reward: 026\n",
      "INFO:root:Episode: 122, Reward: 039\n",
      "INFO:root:Episode: 123, Reward: 074\n",
      "INFO:root:Episode: 124, Reward: 032\n",
      "INFO:root:Episode: 125, Reward: 018\n",
      "INFO:root:Episode: 126, Reward: 018\n",
      "INFO:root:Episode: 127, Reward: 033\n",
      "INFO:root:Episode: 128, Reward: 056\n",
      "INFO:root:Episode: 129, Reward: 027\n",
      "INFO:root:Episode: 130, Reward: 029\n",
      "INFO:root:Episode: 131, Reward: 037\n",
      "INFO:root:Episode: 132, Reward: 048\n",
      "INFO:root:Episode: 133, Reward: 017\n",
      "INFO:root:Episode: 134, Reward: 034\n",
      "INFO:root:Episode: 135, Reward: 018\n",
      "INFO:root:Episode: 136, Reward: 013\n",
      "INFO:root:Episode: 137, Reward: 048\n",
      "INFO:root:Episode: 138, Reward: 012\n",
      "INFO:root:Episode: 139, Reward: 049\n",
      "INFO:root:Episode: 140, Reward: 027\n",
      "INFO:root:Episode: 141, Reward: 024\n",
      "INFO:root:Episode: 142, Reward: 020\n",
      "INFO:root:Episode: 143, Reward: 028\n",
      "INFO:root:Episode: 144, Reward: 012\n",
      "INFO:root:Episode: 145, Reward: 042\n",
      "INFO:root:Episode: 146, Reward: 051\n",
      "INFO:root:Episode: 147, Reward: 054\n",
      "INFO:root:Episode: 148, Reward: 019\n",
      "INFO:root:Episode: 149, Reward: 018\n",
      "INFO:root:Episode: 150, Reward: 014\n",
      "INFO:root:Episode: 151, Reward: 067\n",
      "INFO:root:Episode: 152, Reward: 024\n",
      "INFO:root:Episode: 153, Reward: 019\n",
      "INFO:root:Episode: 154, Reward: 029\n",
      "INFO:root:Episode: 155, Reward: 027\n",
      "INFO:root:Episode: 156, Reward: 019\n",
      "INFO:root:Episode: 157, Reward: 057\n",
      "INFO:root:Episode: 158, Reward: 020\n",
      "INFO:root:Episode: 159, Reward: 032\n",
      "INFO:root:Episode: 160, Reward: 054\n",
      "INFO:root:Episode: 161, Reward: 035\n",
      "INFO:root:Episode: 162, Reward: 059\n",
      "INFO:root:Episode: 163, Reward: 050\n",
      "INFO:root:Episode: 164, Reward: 051\n",
      "INFO:root:Episode: 165, Reward: 034\n",
      "INFO:root:Episode: 166, Reward: 036\n",
      "INFO:root:Episode: 167, Reward: 013\n",
      "INFO:root:Episode: 168, Reward: 057\n",
      "INFO:root:Episode: 169, Reward: 015\n",
      "INFO:root:Episode: 170, Reward: 041\n",
      "INFO:root:Episode: 171, Reward: 022\n",
      "INFO:root:Episode: 172, Reward: 067\n",
      "INFO:root:Episode: 173, Reward: 027\n",
      "INFO:root:Episode: 174, Reward: 044\n",
      "INFO:root:Episode: 175, Reward: 097\n",
      "INFO:root:Episode: 176, Reward: 032\n",
      "INFO:root:Episode: 177, Reward: 029\n",
      "INFO:root:Episode: 178, Reward: 019\n",
      "INFO:root:Episode: 179, Reward: 024\n",
      "INFO:root:Episode: 180, Reward: 033\n",
      "INFO:root:Episode: 181, Reward: 037\n",
      "INFO:root:Episode: 182, Reward: 026\n",
      "INFO:root:Episode: 183, Reward: 021\n",
      "INFO:root:Episode: 184, Reward: 018\n",
      "INFO:root:Episode: 185, Reward: 059\n",
      "INFO:root:Episode: 186, Reward: 032\n",
      "INFO:root:Episode: 187, Reward: 045\n",
      "INFO:root:Episode: 188, Reward: 032\n",
      "INFO:root:Episode: 189, Reward: 046\n",
      "INFO:root:Episode: 190, Reward: 054\n",
      "INFO:root:Episode: 191, Reward: 031\n",
      "INFO:root:Episode: 192, Reward: 024\n",
      "INFO:root:Episode: 193, Reward: 042\n",
      "INFO:root:Episode: 194, Reward: 010\n",
      "INFO:root:Episode: 195, Reward: 032\n",
      "INFO:root:Episode: 196, Reward: 050\n",
      "INFO:root:Episode: 197, Reward: 028\n",
      "INFO:root:Episode: 198, Reward: 035\n",
      "INFO:root:Episode: 199, Reward: 048\n",
      "INFO:root:Episode: 200, Reward: 025\n",
      "INFO:root:Episode: 201, Reward: 043\n",
      "INFO:root:Episode: 202, Reward: 017\n",
      "INFO:root:Episode: 203, Reward: 060\n",
      "INFO:root:Episode: 204, Reward: 019\n",
      "INFO:root:Episode: 205, Reward: 054\n",
      "INFO:root:Episode: 206, Reward: 036\n",
      "INFO:root:Episode: 207, Reward: 076\n",
      "INFO:root:Episode: 208, Reward: 028\n",
      "INFO:root:Episode: 209, Reward: 019\n",
      "INFO:root:Episode: 210, Reward: 059\n",
      "INFO:root:Episode: 211, Reward: 027\n",
      "INFO:root:Episode: 212, Reward: 058\n",
      "INFO:root:Episode: 213, Reward: 027\n",
      "INFO:root:Episode: 214, Reward: 014\n",
      "INFO:root:Episode: 215, Reward: 037\n",
      "INFO:root:Episode: 216, Reward: 031\n",
      "INFO:root:Episode: 217, Reward: 066\n",
      "INFO:root:Episode: 218, Reward: 042\n",
      "INFO:root:Episode: 219, Reward: 021\n",
      "INFO:root:Episode: 220, Reward: 020\n",
      "INFO:root:Episode: 221, Reward: 043\n",
      "INFO:root:Episode: 222, Reward: 014\n",
      "INFO:root:Episode: 223, Reward: 019\n",
      "INFO:root:Episode: 224, Reward: 068\n",
      "INFO:root:Episode: 225, Reward: 016\n",
      "INFO:root:Episode: 226, Reward: 038\n",
      "INFO:root:Episode: 227, Reward: 021\n",
      "INFO:root:Episode: 228, Reward: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Episode: 229, Reward: 070\n",
      "INFO:root:Episode: 230, Reward: 050\n",
      "INFO:root:Episode: 231, Reward: 045\n",
      "INFO:root:Episode: 232, Reward: 083\n",
      "INFO:root:Episode: 233, Reward: 032\n",
      "INFO:root:Episode: 234, Reward: 034\n",
      "INFO:root:Episode: 235, Reward: 058\n",
      "INFO:root:Episode: 236, Reward: 045\n",
      "INFO:root:Episode: 237, Reward: 073\n",
      "INFO:root:Episode: 238, Reward: 063\n",
      "INFO:root:Episode: 239, Reward: 052\n",
      "INFO:root:Episode: 240, Reward: 022\n",
      "INFO:root:Episode: 241, Reward: 020\n",
      "INFO:root:Episode: 242, Reward: 081\n",
      "INFO:root:Episode: 243, Reward: 039\n",
      "INFO:root:Episode: 244, Reward: 042\n",
      "INFO:root:Episode: 245, Reward: 038\n",
      "INFO:root:Episode: 246, Reward: 028\n",
      "INFO:root:Episode: 247, Reward: 036\n",
      "INFO:root:Episode: 248, Reward: 038\n",
      "INFO:root:Episode: 249, Reward: 053\n",
      "INFO:root:Episode: 250, Reward: 025\n",
      "INFO:root:Episode: 251, Reward: 021\n",
      "INFO:root:Episode: 252, Reward: 070\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f3a52631863e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA2CAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrewards_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished training.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total Episode Reward: %d out of 200\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-f8b5af3e17e7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, batch_sz, updates)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-f8b5af3e17e7>\u001b[0m in \u001b[0;36maction_value\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# executes call() under the hood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# a simpler option, will become clear later why we don't use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m           model, mode)\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m       callbacks = cbks.configure_callbacks(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    331\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 332\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    591\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    609\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf20\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m         \u001b[1;34m\"MakeIterator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m         iterator)\n\u001b[0m\u001b[0;32m   2916\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "model = MyModel(num_actions=env.action_space.n)\n",
    "agent = A2CAgent(model)\n",
    "\n",
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training.\")\n",
    "print(\"Total Episode Reward: %d out of 200\" % agent.test(env, True))\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
