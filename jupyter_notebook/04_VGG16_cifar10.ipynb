{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG network (Visual Geometry Group)\n",
    "### very deep convolutional networks for large-scale image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, metrics\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:0.4734 std:0.2516\n",
      "x.shape (50000, 32, 32, 3)\n",
      "x_test.shape (10000, 32, 32, 3)\n",
      "y.shape (50000, 1)\n",
      "y_test.shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "def normalize(X_train, X_test):\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "    mean = np.mean(X_train, axis=(0,1,2,3))\n",
    "    std = np.std(X_train, axis=(0,1,2,3))\n",
    "    print(\"mean:{:.4f} std:{:.4f}\".format(mean,std))\n",
    "    X_train = (X_train-mean) / (std+1e-7)\n",
    "    X_test = (X_test-mean) / (std+1e-7)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_cifar(x,y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    return x, y\n",
    "\n",
    "def data_scale_down(X, Y, X_test, Y_test):\n",
    "    num_train = len(X) // 10\n",
    "    num_test = len(X_test) // 10\n",
    "    X = X[:num_train]\n",
    "    Y = Y[:num_train]\n",
    "    X_test = X_test[:num_test]\n",
    "    Y_test = Y_test[:num_test]\n",
    "    return X, Y, X_test, Y_test\n",
    "\n",
    "def cifar10_dataset():\n",
    "    (x,y), (x_test,y_test) = datasets.cifar10.load_data()\n",
    "    print(\"x.shape\",x.shape)\n",
    "    print(\"x_test.shape\",x_test.shape)\n",
    "    print(\"y.shape\",y.shape)\n",
    "    print(\"y_test.shape\",y_test.shape)\n",
    "    print(\"\\n----Scale Down----\")\n",
    "    x, y, x_test, y_test = data_scale_down(x, y, x_test, y_test)\n",
    "    x, x_test = normalize(x, x_test)\n",
    "    print(\"x.shape\",x.shape)\n",
    "    print(\"x_test.shape\",x_test.shape)\n",
    "    print(\"y.shape\",y.shape)\n",
    "    print(\"y_test.shape\",y_test.shape)\n",
    "    print(\"\\n----Scale Down----\")\n",
    "    \n",
    "    # [b, 1] => [b]\n",
    "    y = tf.squeeze(y, axis=1)\n",
    "    # [b, 10]\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    y_test = tf.squeeze(y_test, axis=1)\n",
    "    y_test = tf.one_hot(y_test, depth=10)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    train_ds = train_ds.map(prepare_cifar).shuffle(50000).batch(256)\n",
    "    \n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "    test_ds = test_ds.map(prepare_cifar).shuffle(10000).batch(256)\n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = cifar10_dataset()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del VGG16\n",
    "class VGG16(models.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(VGG16, self).__init__()\n",
    "        # input_shape => [32, 32, 3]\n",
    "        self.weight_decay = 0.000\n",
    "        self.input_shape_ = input_shape\n",
    "        self.num_classes = 10\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "    def call(self, x):        \n",
    "        return self.model(x)\n",
    "    def create_model(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay), input_shape=self.input_shape_))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))                \n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        model.add(layers.Dense(units=512, activation=\"relu\", kernel_regularizer=regularizers.l2(self.weight_decay)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=0.3))\n",
    "        model.add(layers.Dense(units=self.num_classes, activation=\"softmax\"))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "model = VGG16([32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.1611337661743164 acc: 0.29296875\n",
      "0 40 loss: 2.343963384628296 acc: 0.09648438\n",
      "0 80 loss: 2.355682134628296 acc: 0.10205078\n",
      "0 120 loss: 2.375213384628296 acc: 0.103710935\n",
      "0 160 loss: 2.343963384628296 acc: 0.099609375\n",
      "epoch 0 test acc: 0.1\n",
      "1 0 loss: 2.316619634628296 acc: 0.098230086\n",
      "1 40 loss: 2.367400884628296 acc: 0.09746094\n",
      "1 80 loss: 2.367400884628296 acc: 0.09511719\n",
      "1 120 loss: 2.379119634628296 acc: 0.099804685\n",
      "1 160 loss: 2.383025884628296 acc: 0.1046875\n",
      "epoch 1 test acc: 0.1\n",
      "2 0 loss: 2.351775884628296 acc: 0.10232301\n",
      "2 40 loss: 2.390838384628296 acc: 0.099121094\n",
      "2 80 loss: 2.394744634628296 acc: 0.10195313\n",
      "2 120 loss: 2.347869634628296 acc: 0.102832034\n",
      "2 160 loss: 2.351775884628296 acc: 0.09423828\n",
      "epoch 2 test acc: 0.1\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "def train(epochs):\n",
    "    criteon = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    _metrics = metrics.CategoricalAccuracy()\n",
    "    optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for step, (x,y) in enumerate(train_ds):            \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x)\n",
    "                loss = criteon(y, logits)\n",
    "                # loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n",
    "                # mse_loss = tf.reduce_sum(tf.square(y-logits))\n",
    "                # print(y.shape, logits.shape)\n",
    "                _metrics.update_state(y, logits)\n",
    "                \n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            ## MUST clip gradient here or it will disconverge!\n",
    "            #grads = [tf.clip_by_norm(g, 15) for g in grads]\n",
    "            grads = [tf.clip_by_norm(g, 12) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            if step % 40 == 0:\n",
    "                # for g in grads:\n",
    "                #     print(tf.norm(g).numpy())\n",
    "                print(epoch, step,\n",
    "                      'loss:{:.4f}'.format(float(loss)),\n",
    "                      'acc:{:.4f}'.format(_metrics.result().numpy()))\n",
    "                _metrics.reset_states()\n",
    "                \n",
    "        if epoch % 1 == 0:\n",
    "            metric = metrics.CategoricalAccuracy()\n",
    "            for x,y in test_ds:                \n",
    "                logits = model.predict(x)\n",
    "                ## be careful, these functions can accept y as [b] without warnning.\n",
    "                metric.update_state(y, logits)\n",
    "            print(\"epoch\",epoch,\"test acc:\", metric.result().numpy())\n",
    "            metric.reset_states()\n",
    "    print(\"finish training\")     \n",
    "    \n",
    "train(epochs=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/vgg16s\\assets\n",
      "vgg16s model saved\n"
     ]
    }
   ],
   "source": [
    "#export_path = \"./models/vgg16s\"\n",
    "#model.save(export_path)\n",
    "#tf.saved_model.save(model, export_path)\n",
    "## Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. \n",
    "## It does not work for subclassed models\n",
    "\n",
    "print(\"vgg16 model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
