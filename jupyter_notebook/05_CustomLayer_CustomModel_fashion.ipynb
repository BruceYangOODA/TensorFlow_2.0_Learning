{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del mnist_dataset\n",
    "def prepare_mnist_features_and_labels(x,y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x,y\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x,y),(x_val,y_val) = datasets.fashion_mnist.load_data()\n",
    "    print(\"x.shape\",x.shape)\n",
    "    print(\"y.shape\",y.shape)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    y_val = tf.one_hot(y_val, depth=10)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    ds = ds.map(prepare_mnist_features_and_labels)\n",
    "    ds = ds.shuffle(60000).batch(100)\n",
    "    \n",
    "    ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    ds_val = ds_val.map(prepare_mnist_features_and_labels)\n",
    "    ds_val = ds_val.shuffle(60000).batch(100)\n",
    "    \n",
    "    return ds, ds_val\n",
    "\n",
    "ds_train, ds_val = mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "hidden_1 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "hidden_3 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 239,410\n",
      "Trainable params: 239,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(28,28), name=\"inputs\"))\n",
    "    model.add(layers.Reshape(target_shape=(28*28,), name=\"reshape_1\"))\n",
    "    model.add(layers.Dense(units=200, activation=\"relu\", name=\"hidden_1\"))\n",
    "    model.add(layers.Dense(units=200, activation=\"relu\", name=\"hidden_2\"))\n",
    "    model.add(layers.Dense(units=200, activation=\"relu\", name=\"hidden_3\"))\n",
    "    model.add(layers.Dense(units=10, activation=\"softmax\", name=\"outputs\"))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=optimizers.Adam(lr=0.05),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "# WARNING:tensorflow:Your input ran out of data; interrupting training. \n",
    "# Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches \n",
    "# (in this case, 15000 batches). You may need to use the repeat() function when building your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 2 steps\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 29s 57ms/step - loss: 2.3613 - accuracy: 0.0998 - val_loss: 2.3262 - val_accuracy: 0.1350\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 29s 58ms/step - loss: 2.3613 - accuracy: 0.0998 - val_loss: 2.3562 - val_accuracy: 0.1050\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 34s 68ms/step - loss: 2.3620 - accuracy: 0.0991 - val_loss: 2.3362 - val_accuracy: 0.1250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds_train.repeat(), epochs=3, steps_per_epoch=500,\n",
    "                    validation_data=ds_val.repeat(), validation_steps=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "loss:\t2.3613  2.3613  2.3620  \n",
      "acc:\t0.0998  0.0998  0.0991  \n",
      "v_loss:\t2.3262  2.3562  2.3362  \n",
      "v_acc:\t0.1350  0.1050  0.1250  \n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "loss = [_loss for _loss in history.history['loss'][:]]\n",
    "size = len(loss)\n",
    "print(\"loss:\\t\", end=\"\")\n",
    "for i in range(size):\n",
    "    print(\"%.4f\"%loss[i], \" \",end=\"\")\n",
    "print()\n",
    "acc = [_acc for _acc in history.history['accuracy'][:]]\n",
    "print(\"acc:\\t\", end=\"\")\n",
    "for i in range(size):\n",
    "    print(\"%.4f\"%acc[i], \" \", end=\"\")\n",
    "print()\n",
    "v_loss = [_v_loss for _v_loss in history.history['val_loss'][:]]\n",
    "print(\"v_loss:\\t\", end=\"\")\n",
    "for i in range(size):\n",
    "    print(\"%.4f\"%v_loss[i], \" \", end=\"\")\n",
    "print()\n",
    "v_acc = [_v_acc for _v_acc in history.history['val_accuracy'][:]]\n",
    "print(\"v_acc:\\t\", end=\"\")\n",
    "for i in range(size):\n",
    "    print(\"%.4f\"%v_acc[i], \" \", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 1%，不預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　model = MyLayer([28*28, 200, 200, 10])\n",
    "#del MyLayer\n",
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(MyLayer, self).__init__()\n",
    "        for i in range(1, len(units)):\n",
    "            # w: [input_dim, output_dim]\n",
    "            self.add_variable(name=\"kernel%d\"%i, shape=[units[i-1], units[i]])\n",
    "            # b: [output_dim]\n",
    "            self.add_variable(name=\"bias%d\"%i, shape=[units[i]])\n",
    "            # layer 1 [784,200] [200]\n",
    "            # layer 2 [200,200] [200]\n",
    "            # layer 3 [200,10] [10]            \n",
    "    def call(self, x):\n",
    "        num = len(self.trainable_variables)\n",
    "        #print(\"len(trainable_variables)\",num) output 6 => 3layer weight+bias\n",
    "        x = tf.reshape(x, [-1,28*28])\n",
    "        for i in range(0, num, 2):\n",
    "            x = tf.matmul(x, self.trainable_variables[i]) + self.trainable_variables[i+1]\n",
    "            #print(self.trainable_variables[i].name)\n",
    "        return x\n",
    "# Please use `layer.add_weight` method instead add_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x/y shape: (60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "#del mnist_dataset\n",
    "def mnist_dataset():\n",
    "    (x, y), _ = datasets.fashion_mnist.load_data()\n",
    "\n",
    "    print('x/y shape:', x.shape, y.shape)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(prepare_mnist_features_and_labels)\n",
    "    ds = ds.take(20000).shuffle(20000).batch(100)\n",
    "    return ds\n",
    "train_dataset = mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (60000, 28, 28)\n",
      "y.shape (60000,)\n"
     ]
    }
   ],
   "source": [
    "#del model\n",
    "model = MyLayer([28*28, 200, 200, 10])\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel1:0 (784, 200)\n",
      "bias1:0 (200,)\n",
      "kernel2:0 (200, 200)\n",
      "bias2:0 (200,)\n",
      "kernel3:0 (200, 10)\n",
      "bias3:0 (10,)\n"
     ]
    }
   ],
   "source": [
    "for p in model.trainable_variables:\n",
    "    print(p.name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": loss 0.5796629 ; accuracy 0.79\n",
      ": loss 0.41130623 ; accuracy 0.83\n",
      "finish training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=102111, shape=(), dtype=float32, numpy=0.41130623>,\n",
       " <tf.Tensor: id=102178, shape=(), dtype=float32, numpy=0.83>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy\n",
    "def train(train_ds):\n",
    "    #train_ds, val_ds = mnist_dataset()\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0    \n",
    "    for step, (x,y) in enumerate(train_ds):\n",
    "        loss, accuracy = train_one_step(model, optimizer, x, y)\n",
    "        if (step+1)%100 == 0:\n",
    "            print(': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "    print(\"finish training\")\n",
    "    return loss, accuracy\n",
    "\n",
    "train(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x/y shape: (60000, 28, 28) (60000,)\n",
      "sample: (100, 28, 28) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "#del mnist_dataset\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_val, y_val) = datasets.fashion_mnist.load_data()\n",
    "    print('x/y shape:', x.shape, y.shape)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    y_val = tf.one_hot(y_val, depth=10)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(prepare_mnist_features_and_labels)\n",
    "    ds = ds.shuffle(60000).batch(100)\n",
    "\n",
    "    ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    ds_val = ds_val.map(prepare_mnist_features_and_labels)\n",
    "    ds_val = ds_val.shuffle(10000).batch(100)\n",
    "\n",
    "    sample = next(iter(ds))\n",
    "    print('sample:', sample[0].shape, sample[1].shape)\n",
    "\n",
    "    return ds,ds_val\n",
    "\n",
    "train_dataset, val_dataset = mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 2 steps\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 25s 51ms/step - loss: 1.7450 - accuracy: 0.7239 - val_loss: 1.6676 - val_accuracy: 0.8000\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 20s 39ms/step - loss: 1.6753 - accuracy: 0.7864 - val_loss: 1.6818 - val_accuracy: 0.7750\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 21s 41ms/step - loss: 1.6722 - accuracy: 0.7896 - val_loss: 1.6095 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23490b36438>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del MyModel\n",
    "class MyModel(models.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = layers.Dense(units=200, activation=\"relu\", name=\"hidden_1\")\n",
    "        self.layer2 = layers.Dense(units=200, activation=\"relu\", name=\"hidden_1\")\n",
    "        self.layer3 = layers.Dense(units=10, activation=\"softmax\", name=\"outputs\")\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = tf.reshape(x, [-1,28*28])\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "#del model\n",
    "model = MyModel()\n",
    "\n",
    "model.compile(loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=optimizers.Adam(1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=3, steps_per_epoch=500, verbose=1,\n",
    "              validation_data=val_dataset.repeat(),\n",
    "              validation_steps=2)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
