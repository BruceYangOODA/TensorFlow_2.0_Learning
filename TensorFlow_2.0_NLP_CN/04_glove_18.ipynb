{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os, csv, random, math\n",
    "import bz2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import time\n",
    "from math import ceil\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/wikipedia2text-extracted.txt.bz2'\n",
    "vocabulary_size = 5000 # origin 50000\n",
    "\n",
    "batch_size = 24 # origin 128\n",
    "embedding_size = 24 # origin 128\n",
    "window_size = 2 # origin 4\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 50\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "num_steps = 100001 # origin 100001\n",
    "\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "epsilon = 1 # used for the stability of log in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "...00/17 20:09:59\n",
      "...01/17 20:10:00\n",
      "...02/17 20:10:01\n",
      "...03/17 20:10:02\n",
      "...04/17 20:10:04\n",
      "...05/17 20:10:05\n",
      "...06/17 20:10:06\n",
      "...07/17 20:10:07\n",
      "...08/17 20:10:09\n",
      "...09/17 20:10:10\n",
      "...10/17 20:10:11\n",
      "...11/17 20:10:12\n",
      "...12/17 20:10:14\n",
      "...13/17 20:10:15\n",
      "...14/17 20:10:16\n",
      "...15/17 20:10:18\n",
      "...16/17 20:10:20\n",
      "...17/17 20:10:22\n",
      "Data size 3361213\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n",
      "Cost 23.93 seconds\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):    \n",
    "    with bz2.BZ2File(filename) as f:        \n",
    "        data = []\n",
    "        file_size = os.stat(filename).st_size\n",
    "        chunk_size = 1024 * 1024 # reading 1 MB at a time as the dataset is moderately large\n",
    "        print('Reading data...')\n",
    "\n",
    "        for i in range(ceil(file_size//chunk_size)+1):        \n",
    "            print('...%02d/%02d'%(i,ceil(file_size//chunk_size)),\n",
    "                  time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "            bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
    "            file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "            file_string = file_string.lower()\n",
    "            # tokenizes a string to words residing in a list\n",
    "            file_string = nltk.word_tokenize(file_string)\n",
    "            data.extend(file_string)\n",
    "    \n",
    "    return data\n",
    "\n",
    "tStart = time.time()\n",
    "words = read_data(filename)\n",
    "token_count = len(words)\n",
    "print('Data size %d' % token_count)\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])\n",
    "tEnd = time.time()\n",
    "print('Cost %.2f seconds' % (tEnd - tStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 500328], ('the', 226893), (',', 184013), ('.', 120919), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 0, 223, 4, 0, 4459, 26, 0]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]  \n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)    \n",
    "    data = list()\n",
    "    unk_count = 0 \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    assert len(dictionary) == vocabulary_size\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "        \n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'UNK', 'set', 'of', 'UNK', 'aimed']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'UNK', 'UNK', 'UNK', 'UNK']\n",
      "    labels: ['propaganda', 'is', 'UNK', 'set', 'is', 'a', 'set', 'of']\n",
      "    weights: [0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5]\n",
      "\n",
      "with window_size = 4:\n",
      "    batch: ['set', 'set', 'set', 'set', 'set', 'set', 'set', 'set']\n",
      "    labels: ['propaganda', 'is', 'a', 'UNK', 'of', 'UNK', 'aimed', 'at']\n",
      "    weights: [0.25, 0.33333334, 0.5, 1.0, 1.0, 0.5, 0.33333334, 0.25]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "    # data_index is updated by 1 everytime we read a data point\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    weights = np.ndarray(shape=(batch_size), dtype=np.float32)\n",
    "    span = 2 * window_size + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)     \n",
    "    num_samples = 2*window_size \n",
    "    for i in range(batch_size // num_samples):    \n",
    "        k=0        \n",
    "        for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "            batch[i*num_samples+k] = buffer[window_size]\n",
    "            labels[i*num_samples+k, 0] = buffer[j]\n",
    "            weights[i*num_samples+k] = abs(1.0/(j-window_size))\n",
    "            k+=1\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels, weights\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "for window_size in [2, 4]:\n",
    "    data_index = 0\n",
    "    batch, labels, weights = generate_batch(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' % window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "    print('    weights:', [w for w in weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Word Co-Occurance Matrix\n",
    "Why GloVe shine above context window based method is that it employs global statistics of the corpus in to the model (according to authors). This is done by using information from the word co-occurance matrix to optimize the word vectors. Basically, the X(i,j) entry of the co-occurance matrix says how frequent word i to appear near j. We also use a weighting mechanishm to give more weight to words close together than to ones further-apart (from experiments section of the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5000)\n",
      "Running 420151 iterations to compute the co-occurance matrix\n",
      "\tFinished 100000 iterations\n",
      "\tFinished 200000 iterations\n",
      "\tFinished 300000 iterations\n",
      "\tFinished 400000 iterations\n",
      "Sample chunks of co-occurance matrix\n",
      "\n",
      "Target Word: \"lives\" 20:30:16\n",
      "Context word:\"of\"(id:4,count:18.50), \"their\"(id:41,count:15.50), \"the\"(id:1,count:14.50), \"UNK\"(id:0,count:13.00), \".\"(id:3,count:11.00), \"and\"(id:5,count:8.50), \"in\"(id:6,count:8.50), \",\"(id:2,count:7.50), \"population\"(id:123,count:4.00), \"on\"(id:18,count:2.50), \n",
      "\n",
      "Target Word: \"surface\" 20:30:16\n",
      "Context word:\"the\"(id:1,count:48.50), \"UNK\"(id:0,count:33.00), \".\"(id:3,count:22.50), \"of\"(id:4,count:22.00), \",\"(id:2,count:14.00), \"'s\"(id:19,count:11.50), \"to\"(id:7,count:9.00), \"a\"(id:8,count:8.00), \"and\"(id:5,count:6.00), \"on\"(id:18,count:5.00), \n",
      "\n",
      "Target Word: \"workers\" 20:30:16\n",
      "Context word:\"UNK\"(id:0,count:23.00), \",\"(id:2,count:16.00), \"of\"(id:4,count:11.50), \".\"(id:3,count:10.00), \"the\"(id:1,count:8.00), \"from\"(id:21,count:5.00), \"'\"(id:80,count:5.00), \"and\"(id:5,count:5.00), \"foreign\"(id:459,count:4.00), \"to\"(id:7,count:3.00), \n",
      "\n",
      "Target Word: \"patron\" 20:30:16\n",
      "Context word:\",\"(id:2,count:7.50), \"of\"(id:4,count:7.00), \"UNK\"(id:0,count:4.50), \"a\"(id:8,count:4.00), \"the\"(id:1,count:3.50), \"his\"(id:25,count:3.00), \"and\"(id:5,count:2.50), \"saint\"(id:1197,count:2.00), \"king\"(id:226,count:1.50), \"'s\"(id:19,count:1.00), \n",
      "\n",
      "Target Word: \"situated\" 20:30:16\n",
      "Context word:\"is\"(id:9,count:20.00), \"UNK\"(id:0,count:11.50), \"in\"(id:6,count:11.00), \",\"(id:2,count:10.00), \"the\"(id:1,count:8.00), \"on\"(id:18,count:7.00), \"at\"(id:26,count:4.00), \"to\"(id:7,count:3.50), \"are\"(id:23,count:2.50), \"close\"(id:573,count:2.00), \n",
      "\n",
      "Target Word: \"billy\" 20:30:16\n",
      "Context word:\"UNK\"(id:0,count:18.50), \",\"(id:2,count:9.00), \".\"(id:3,count:3.00), \"'s\"(id:19,count:2.50), \"as\"(id:10,count:2.00), \"the\"(id:1,count:1.50), \"and\"(id:5,count:1.50), \"cross\"(id:1677,count:1.00), \"``\"(id:22,count:1.00), \"''\"(id:20,count:1.00), \n",
      "\n",
      "Target Word: \"s\" 20:30:16\n",
      "Context word:\"â€™\"(id:247,count:189.00), \"UNK\"(id:0,count:81.00), \",\"(id:2,count:13.00), \".\"(id:3,count:7.50), \"and\"(id:5,count:7.00), \"the\"(id:1,count:6.50), \"is\"(id:9,count:6.00), \"*\"(id:3806,count:5.50), \"was\"(id:11,count:5.00), \"picasso\"(id:2747,count:4.00), \n",
      "\n",
      "Target Word: \"to\" 20:30:16\n",
      "Context word:\"UNK\"(id:0,count:8732.00), \"the\"(id:1,count:3821.00), \"be\"(id:30,count:1088.00), \",\"(id:2,count:938.50), \"a\"(id:8,count:915.50), \".\"(id:3,count:559.00), \"and\"(id:5,count:541.00), \"in\"(id:6,count:420.00), \"is\"(id:9,count:382.00), \"was\"(id:11,count:363.00), \n",
      "\n",
      "Target Word: \"a\" 20:30:16\n",
      "Context word:\"UNK\"(id:0,count:7294.00), \",\"(id:2,count:1933.00), \"of\"(id:4,count:1889.00), \"as\"(id:10,count:1215.00), \".\"(id:3,count:1047.50), \"in\"(id:6,count:978.00), \"to\"(id:7,count:914.50), \"is\"(id:9,count:811.50), \"and\"(id:5,count:699.00), \"with\"(id:17,count:567.50), \n",
      "\n",
      "Target Word: \"is\" 20:30:16\n",
      "Context word:\"UNK\"(id:0,count:3064.00), \"the\"(id:1,count:1295.00), \",\"(id:2,count:879.50), \"a\"(id:8,count:837.50), \"it\"(id:24,count:715.00), \".\"(id:3,count:608.50), \"to\"(id:7,count:369.50), \"that\"(id:16,count:361.50), \"and\"(id:5,count:332.00), \"there\"(id:63,count:303.50), \n",
      "Cost 52.34 seconds\n"
     ]
    }
   ],
   "source": [
    "cooc_data_index = 0\n",
    "dataset_size = len(data) # We iterate through the full text\n",
    "\n",
    "# The sparse matrix that stores the word co-occurences\n",
    "cooc_mat = lil_matrix((vocabulary_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "print(cooc_mat.shape)\n",
    "def generate_cooc(batch_size,skip_window):\n",
    "    data_index = 0\n",
    "    print('Running %d iterations to compute the co-occurance matrix'%(dataset_size//batch_size))\n",
    "    for i in range(dataset_size//batch_size):\n",
    "        # Printing progress\n",
    "        if i>0 and i%100000==0:\n",
    "            print('\\tFinished %d iterations'%i)\n",
    "        batch, labels, weights = generate_batch(batch_size, skip_window)\n",
    "        labels = labels.reshape(-1)  \n",
    "        for inp,lbl,w in zip(batch,labels,weights):            \n",
    "            cooc_mat[inp,lbl] += (1.0*w)\n",
    "\n",
    "tStart = time.time()\n",
    "# Generate the matrix\n",
    "generate_cooc(8,skip_window)    \n",
    "print('Sample chunks of co-occurance matrix')\n",
    "# Basically calculates the highest cooccurance of several chosen word\n",
    "for i in range(10):\n",
    "    idx_target = i    \n",
    "    # get the ith row of the sparse matrix and make it dense\n",
    "    ith_row = cooc_mat.getrow(idx_target)     \n",
    "    ith_row_dense = ith_row.toarray('C').reshape(-1)            \n",
    "    # select target words only with a reasonable words around it.\n",
    "    while np.sum(ith_row_dense)<10 or np.sum(ith_row_dense)>50000:\n",
    "        # Choose a random word\n",
    "        idx_target = np.random.randint(0,vocabulary_size)\n",
    "        ith_row = cooc_mat.getrow(idx_target) \n",
    "        ith_row_dense = ith_row.toarray('C').reshape(-1)    \n",
    "        \n",
    "    print('\\nTarget Word: \"%s\"'%reverse_dictionary[idx_target], time.strftime('%H:%M:%S',time.localtime()))\n",
    "        \n",
    "    sort_indices = np.argsort(ith_row_dense).reshape(-1) # indices with highest count of ith_row_dense\n",
    "    sort_indices = np.flip(sort_indices,axis=0) # reverse the array (to get max values to the start)\n",
    "\n",
    "    # printing several context words to make sure cooc_mat is correct\n",
    "    print('Context word:',end='')\n",
    "    for j in range(10):        \n",
    "        idx_context = sort_indices[j]       \n",
    "        print('\"%s\"(id:%d,count:%.2f), '%(reverse_dictionary[idx_context],idx_context,ith_row_dense[idx_context]),end='')\n",
    "    print()\n",
    "tEnd = time.time()\n",
    "print('Cost %.2f seconds'%(tEnd-tStart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "in_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "in_bias_embeddings = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')\n",
    "out_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "out_bias_embeddings = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')\n",
    "\n",
    "embed_in = tf.nn.embedding_lookup(in_embeddings, train_dataset)\n",
    "embed_out = tf.nn.embedding_lookup(out_embeddings, train_labels)\n",
    "embed_bias_in = tf.nn.embedding_lookup(in_bias_embeddings,train_dataset)\n",
    "embed_bias_out = tf.nn.embedding_lookup(out_bias_embeddings,train_labels)\n",
    "# weights used in the cost function\n",
    "weights_x = tf.placeholder(tf.float32,shape=[batch_size],name='weights_x') \n",
    "# Cooccurence value for that position\n",
    "x_ij = tf.placeholder(tf.float32,shape=[batch_size],name='x_ij')\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    weights_x * (tf.reduce_sum(embed_in*embed_out,axis=1) + embed_bias_in + embed_bias_out - tf.log(epsilon+x_ij))**2)\n",
    "\n",
    "## Calculating Word Similarities\n",
    "embeddings = (in_embeddings + out_embeddings)/2.0\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 20:48:31\n",
      "Average loss at step 0: 31.904291\n",
      "20:48:32\n",
      "valid_size00 :  Nearest to which: felix, rail, earth, church, biological, horizontal, hero, provincial,\n",
      "valid_size01 :  Nearest to as: supported, devoted, test, claims, grand, japan, develop, decline,\n",
      "valid_size02 :  Nearest to .: UNK, it, decade, railway, or, takes, description, particles,\n",
      "valid_size03 :  Nearest to were: planets, released, -, neighboring, still, votes, gaining, data,\n",
      "valid_size04 :  Nearest to an: reasons, precipitation, seems, symbols, experiments, find, station, heaven,\n",
      "valid_size05 :  Nearest to of: decay, blue, base, communication, castle, column, religious, competition,\n",
      "valid_size06 :  Nearest to from: tested, techniques, egyptians, ran, islamic, reading, 1998, assembled,\n",
      "valid_size07 :  Nearest to and: liberalism, d, 10, army, experts, beat, amsterdam, ',\n",
      "valid_size08 :  Nearest to city: temple, milk, guns, programme, promised, victor, millennium, satellite,\n",
      "valid_size09 :  Nearest to most: image, mill, systematic, port, havana, millennium, exception, 1930,\n",
      "valid_size10 :  Nearest to he: seen, largest, founder, austria, temple, requiring, think, ongoing,\n",
      "valid_size11 :  Nearest to for: present-day, yellow, mechanism, fairly, experiments, scene, observe, says,\n",
      "valid_size12 :  Nearest to :: thirteen, distinctive, directions, carey, carried, decrease, cane, springsteen,\n",
      "valid_size13 :  Nearest to in: treatments, el, wounded, perception, later, indeed, treaty, 4th,\n",
      "valid_size14 :  Nearest to '': tens, associate, holds, treat, february, fertile, professional, elite,\n",
      "valid_size15 :  Nearest to is: without, tallinn, crossing, decade, socialism, ammunition, synthesis, favour,\n",
      "Average loss at step 2000: 0.752940\n",
      "Average loss at step 4000: 0.198187\n",
      "Average loss at step 6000: 0.135583\n",
      "Average loss at step 8000: 0.107523\n",
      "Average loss at step 10000: 0.089838\n",
      "20:48:40\n",
      "valid_size00 :  Nearest to which: who, was, ,, however, but, and, ;, were,\n",
      "valid_size01 :  Nearest to as: such, a, well, known, ``, that, far, republicans,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, and, 's, for, ,,\n",
      "valid_size03 :  Nearest to were: are, was, he, also, who, they, ), she,\n",
      "valid_size04 :  Nearest to an: a, with, then, from, by, UNK, shanghai, soviet,\n",
      "valid_size05 :  Nearest to of: the, in, ., UNK, a, for, and, from,\n",
      "valid_size06 :  Nearest to from: the, UNK, on, of, in, to, a, 's,\n",
      "valid_size07 :  Nearest to and: ,, UNK, for, with, the, in, ;, a,\n",
      "valid_size08 :  Nearest to city: first, following, of, country, region, distinguished, his, the,\n",
      "valid_size09 :  Nearest to most: hindi, all, involve, ad, easier, objective, applied, certainly,\n",
      "valid_size10 :  Nearest to he: are, were, who, this, however, where, they, saint,\n",
      "valid_size11 :  Nearest to for: and, UNK, the, ,, in, of, 's, .,\n",
      "valid_size12 :  Nearest to :: see, (, '', ``, or, ), 1932, intake,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, of, and, ,, for, 's,\n",
      "valid_size14 :  Nearest to '': ), (, :, ;, ``, or, ., 's,\n",
      "valid_size15 :  Nearest to is: was, it, by, also, that, are, they, were,\n",
      "Average loss at step 12000: 0.084206\n",
      "Average loss at step 14000: 0.073995\n",
      "Average loss at step 16000: 0.065137\n",
      "Average loss at step 18000: 0.057071\n",
      "Average loss at step 20000: 0.057310\n",
      "20:48:49\n",
      "valid_size00 :  Nearest to which: was, ,, however, but, and, who, were, that,\n",
      "valid_size01 :  Nearest to as: a, such, well, an, known, with, ``, UNK,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, ,, ;, and, 's,\n",
      "valid_size03 :  Nearest to were: are, was, also, is, it, which, however, that,\n",
      "valid_size04 :  Nearest to an: a, with, soviet, as, UNK, over, ,, then,\n",
      "valid_size05 :  Nearest to of: the, ., UNK, 's, in, for, a, and,\n",
      "valid_size06 :  Nearest to from: to, on, the, UNK, in, ., 's, of,\n",
      "valid_size07 :  Nearest to and: ,, UNK, for, ;, in, the, with, .,\n",
      "valid_size08 :  Nearest to city: first, region, country, of, island, following, distinguished, one,\n",
      "valid_size09 :  Nearest to most: hindi, of, city, population, all, surface, 's, shanghai,\n",
      "valid_size10 :  Nearest to he: they, so, who, it, which, however, where, when,\n",
      "valid_size11 :  Nearest to for: and, UNK, the, ,, ;, in, of, .,\n",
      "valid_size12 :  Nearest to :: (, see, '', ``, include, ), ', or,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, of, for, ;,\n",
      "valid_size14 :  Nearest to '': ), ;, (, :, ``, ', ., or,\n",
      "valid_size15 :  Nearest to is: was, are, it, that, also, by, were, which,\n",
      "Average loss at step 22000: 0.050685\n",
      "Average loss at step 24000: 0.048238\n",
      "Average loss at step 26000: 0.054033\n",
      "Average loss at step 28000: 0.044856\n",
      "Average loss at step 30000: 0.044471\n",
      "20:48:58\n",
      "valid_size00 :  Nearest to which: was, ,, but, is, it, he, however, were,\n",
      "valid_size01 :  Nearest to as: a, such, well, known, ``, an, UNK, with,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, 's, ,, and, ;,\n",
      "valid_size03 :  Nearest to were: are, was, also, is, they, which, it, had,\n",
      "valid_size04 :  Nearest to an: with, a, then, as, UNK, for, from, ,,\n",
      "valid_size05 :  Nearest to of: the, 's, UNK, ., in, and, for, its,\n",
      "valid_size06 :  Nearest to from: to, UNK, for, the, on, in, 's, .,\n",
      "valid_size07 :  Nearest to and: ,, UNK, in, for, the, ;, with, .,\n",
      "valid_size08 :  Nearest to city: region, first, following, country, all, area, life, church,\n",
      "valid_size09 :  Nearest to most: of, population, much, all, rest, shanghai, one, surrounding,\n",
      "valid_size10 :  Nearest to he: they, who, she, when, which, it, were, however,\n",
      "valid_size11 :  Nearest to for: UNK, and, in, ,, the, from, his, .,\n",
      "valid_size12 :  Nearest to :: (, '', ``, ', ), ;, see, or,\n",
      "valid_size13 :  Nearest to in: ., UNK, the, and, ,, for, ;, 's,\n",
      "valid_size14 :  Nearest to '': ), ;, (, ', :, ``, ., or,\n",
      "valid_size15 :  Nearest to is: was, it, are, also, that, by, which, were,\n",
      "Average loss at step 32000: 0.043710\n",
      "Average loss at step 34000: 0.040180\n",
      "Average loss at step 36000: 0.038768\n",
      "Average loss at step 38000: 0.036187\n",
      "Average loss at step 40000: 0.034904\n",
      "20:49:07\n",
      "valid_size00 :  Nearest to which: was, he, when, ,, is, but, it, were,\n",
      "valid_size01 :  Nearest to as: a, such, well, an, known, ``, with, UNK,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, 's, and, ,, for,\n",
      "valid_size03 :  Nearest to were: was, are, also, is, it, they, may, which,\n",
      "valid_size04 :  Nearest to an: with, a, as, UNK, ,, then, and, to,\n",
      "valid_size05 :  Nearest to of: the, 's, ., UNK, in, a, and, for,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, in, ., of, for,\n",
      "valid_size07 :  Nearest to and: ,, UNK, in, with, the, ;, for, .,\n",
      "valid_size08 :  Nearest to city: region, first, country, u.s., capital, area, government, world,\n",
      "valid_size09 :  Nearest to most: rest, all, population, of, parts, state, surface, hindi,\n",
      "valid_size10 :  Nearest to he: when, which, she, they, was, who, were, it,\n",
      "valid_size11 :  Nearest to for: UNK, and, in, his, the, ., ,, its,\n",
      "valid_size12 :  Nearest to :: (, ``, include, '', see, ;, president, ',\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, of, for, 's,\n",
      "valid_size14 :  Nearest to '': ), ;, (, ', ., ``, :, UNK,\n",
      "valid_size15 :  Nearest to is: was, it, are, also, by, were, which, that,\n",
      "Average loss at step 42000: 0.035255\n",
      "Average loss at step 44000: 0.031692\n",
      "Average loss at step 46000: 0.032138\n",
      "Average loss at step 48000: 0.029871\n",
      "Average loss at step 50000: 0.029519\n",
      "20:49:16\n",
      "valid_size00 :  Nearest to which: was, he, but, ,, it, is, when, also,\n",
      "valid_size01 :  Nearest to as: such, a, well, known, an, ``, UNK, ,,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, ,, and, for, at,\n",
      "valid_size03 :  Nearest to were: are, may, was, he, also, they, had, would,\n",
      "valid_size04 :  Nearest to an: a, with, as, UNK, ,, and, for, to,\n",
      "valid_size05 :  Nearest to of: the, ., UNK, 's, in, and, for, a,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, in, ., by, for,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, in, with, ., for, a,\n",
      "valid_size08 :  Nearest to city: region, country, area, first, state, of, capital, during,\n",
      "valid_size09 :  Nearest to most: all, rest, state, parts, of, population, one, area,\n",
      "valid_size10 :  Nearest to he: when, she, which, were, they, however, was, it,\n",
      "valid_size11 :  Nearest to for: UNK, the, their, and, his, in, its, .,\n",
      "valid_size12 :  Nearest to :: (, ``, '', include, ;, ', see, under,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, of, for, at,\n",
      "valid_size14 :  Nearest to '': ), (, ', ;, :, ., in, ``,\n",
      "valid_size15 :  Nearest to is: was, it, also, are, by, which, that, has,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: 0.027225\n",
      "Average loss at step 54000: 0.028118\n",
      "Average loss at step 56000: 0.027580\n",
      "Average loss at step 58000: 0.028751\n",
      "Average loss at step 60000: 0.028986\n",
      "20:49:25\n",
      "valid_size00 :  Nearest to which: was, he, it, ,, but, is, when, also,\n",
      "valid_size01 :  Nearest to as: such, a, well, an, known, ``, UNK, with,\n",
      "valid_size02 :  Nearest to .: the, in, UNK, of, ,, and, for, ),\n",
      "valid_size03 :  Nearest to were: was, are, may, is, had, have, would, by,\n",
      "valid_size04 :  Nearest to an: a, with, as, UNK, ,, and, for, to,\n",
      "valid_size05 :  Nearest to of: the, ., 's, UNK, in, state, a, and,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, by, for, ., in,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, ., in, ;, for, with,\n",
      "valid_size08 :  Nearest to city: region, country, state, first, during, capital, government, 's,\n",
      "valid_size09 :  Nearest to most: state, of, all, population, area, parts, modern, rest,\n",
      "valid_size10 :  Nearest to he: when, she, which, was, they, it, had, were,\n",
      "valid_size11 :  Nearest to for: UNK, and, the, ., in, ,, a, his,\n",
      "valid_size12 :  Nearest to :: (, ``, '', include, ', see, under, through,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, ,, and, of, for, ),\n",
      "valid_size14 :  Nearest to '': ), (, ;, ', ., ``, :, UNK,\n",
      "valid_size15 :  Nearest to is: was, it, also, are, were, by, which, has,\n",
      "Average loss at step 62000: 0.027001\n",
      "Average loss at step 64000: 0.024790\n",
      "Average loss at step 66000: 0.025501\n",
      "Average loss at step 68000: 0.024266\n",
      "Average loss at step 70000: 0.025399\n",
      "20:49:34\n",
      "valid_size00 :  Nearest to which: was, it, he, ,, is, but, when, by,\n",
      "valid_size01 :  Nearest to as: such, a, well, an, known, ``, UNK, ,,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, and, for, ,, 's,\n",
      "valid_size03 :  Nearest to were: are, may, was, had, they, have, would, also,\n",
      "valid_size04 :  Nearest to an: a, with, as, UNK, ,, for, to, and,\n",
      "valid_size05 :  Nearest to of: the, ., UNK, 's, in, and, for, state,\n",
      "valid_size06 :  Nearest to from: on, to, UNK, the, for, 's, their, .,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, in, with, ., for, a,\n",
      "valid_size08 :  Nearest to city: region, country, government, first, state, capital, house, area,\n",
      "valid_size09 :  Nearest to most: state, of, all, area, population, law, modern, parts,\n",
      "valid_size10 :  Nearest to he: when, she, which, they, was, it, had, where,\n",
      "valid_size11 :  Nearest to for: UNK, the, ., in, and, its, with, his,\n",
      "valid_size12 :  Nearest to :: (, include, ``, '', ;, see, president, about,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, for, of, by,\n",
      "valid_size14 :  Nearest to '': ), (, ;, ', ., ``, :, UNK,\n",
      "valid_size15 :  Nearest to is: it, was, also, are, which, by, that, has,\n",
      "Average loss at step 72000: 0.024051\n",
      "Average loss at step 74000: 0.024127\n",
      "Average loss at step 76000: 0.022205\n",
      "Average loss at step 78000: 0.022437\n",
      "Average loss at step 80000: 0.022505\n",
      "20:49:43\n",
      "valid_size00 :  Nearest to which: was, it, is, ,, he, when, but, were,\n",
      "valid_size01 :  Nearest to as: such, a, well, an, ``, known, UNK, ,,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, 's, and, for, ,,\n",
      "valid_size03 :  Nearest to were: are, may, was, they, which, would, had, by,\n",
      "valid_size04 :  Nearest to an: a, with, as, for, UNK, ,, and, to,\n",
      "valid_size05 :  Nearest to of: the, ., 's, UNK, in, and, state, for,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, by, ., in, and,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, in, ., with, ;, for,\n",
      "valid_size08 :  Nearest to city: region, country, state, capital, government, first, 's, during,\n",
      "valid_size09 :  Nearest to most: state, law, all, of, modern, parts, one, areas,\n",
      "valid_size10 :  Nearest to he: when, she, which, they, had, was, who, where,\n",
      "valid_size11 :  Nearest to for: UNK, in, the, ., and, its, a, his,\n",
      "valid_size12 :  Nearest to :: include, (, ``, see, through, ;, '', about,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, for, of, at,\n",
      "valid_size14 :  Nearest to '': ), ;, (, ., ', ``, UNK, in,\n",
      "valid_size15 :  Nearest to is: it, was, also, are, which, by, that, has,\n",
      "Average loss at step 82000: 0.022557\n",
      "Average loss at step 84000: 0.021135\n",
      "Average loss at step 86000: 0.021117\n",
      "Average loss at step 88000: 0.019757\n",
      "Average loss at step 90000: 0.020109\n",
      "20:49:52\n",
      "valid_size00 :  Nearest to which: was, he, when, it, ,, is, by, but,\n",
      "valid_size01 :  Nearest to as: such, a, well, ``, an, UNK, known, ,,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, 's, and, ,, ),\n",
      "valid_size03 :  Nearest to were: are, may, was, would, had, by, he, have,\n",
      "valid_size04 :  Nearest to an: a, with, as, UNK, for, ,, and, to,\n",
      "valid_size05 :  Nearest to of: the, 's, ., UNK, in, and, for, state,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, about, 's, them, and,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, in, ., with, for, ),\n",
      "valid_size08 :  Nearest to city: region, country, state, capital, first, government, largest, world,\n",
      "valid_size09 :  Nearest to most: law, state, of, modern, all, parts, population, area,\n",
      "valid_size10 :  Nearest to he: when, she, which, had, was, they, were, would,\n",
      "valid_size11 :  Nearest to for: UNK, and, the, ., in, ,, with, a,\n",
      "valid_size12 :  Nearest to :: include, (, ``, '', see, through, or, about,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, ), 's, for,\n",
      "valid_size14 :  Nearest to '': ), ;, (, ., ', ``, in, UNK,\n",
      "valid_size15 :  Nearest to is: it, was, also, by, which, are, that, has,\n",
      "Average loss at step 92000: 0.019874\n",
      "Average loss at step 94000: 0.019151\n",
      "Average loss at step 96000: 0.019813\n",
      "Average loss at step 98000: 0.020421\n",
      "Average loss at step 100000: 0.019256\n",
      "20:50:00\n",
      "valid_size00 :  Nearest to which: was, when, ,, he, it, is, in, but,\n",
      "valid_size01 :  Nearest to as: such, a, well, an, ``, UNK, ,, known,\n",
      "valid_size02 :  Nearest to .: in, the, UNK, of, and, ,, ;, for,\n",
      "valid_size03 :  Nearest to were: are, may, would, they, was, had, that, have,\n",
      "valid_size04 :  Nearest to an: a, with, as, for, UNK, and, ,, to,\n",
      "valid_size05 :  Nearest to of: the, 's, ., UNK, in, and, state, a,\n",
      "valid_size06 :  Nearest to from: to, on, UNK, the, by, for, ., over,\n",
      "valid_size07 :  Nearest to and: ,, UNK, the, in, ., with, for, ;,\n",
      "valid_size08 :  Nearest to city: region, country, capital, government, state, of, 's, work,\n",
      "valid_size09 :  Nearest to most: state, law, modern, of, population, all, government, one,\n",
      "valid_size10 :  Nearest to he: when, she, which, was, they, had, it, would,\n",
      "valid_size11 :  Nearest to for: UNK, and, the, ., in, with, to, their,\n",
      "valid_size12 :  Nearest to :: include, (, ``, '', see, about, through, under,\n",
      "valid_size13 :  Nearest to in: ., the, UNK, and, ,, of, ;, for,\n",
      "valid_size14 :  Nearest to '': ), ;, (, ., ', ``, in, :,\n",
      "valid_size15 :  Nearest to is: it, was, also, which, are, by, that, has,\n",
      "Cost 89.02 seconds\n"
     ]
    }
   ],
   "source": [
    "glove_loss = []\n",
    "average_loss = 0\n",
    "tStart= time.time()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized',time.strftime('%H:%M:%S',time.localtime()))    \n",
    "    for step in range(num_steps):        \n",
    "        batch_data, batch_labels, batch_weights = generate_batch(batch_size, skip_window)         \n",
    "        batch_weights = [] # weighting used in the loss function\n",
    "        batch_xij = [] # weighted frequency of finding i near j        \n",
    "        # Compute the weights for each datapoint in the batch\n",
    "        for inp,lbl in zip(batch_data,batch_labels.reshape(-1)):     \n",
    "            point_weight = (cooc_mat[inp,lbl]/100.0)**0.75 if cooc_mat[inp,lbl]<100.0 else 1.0 \n",
    "            batch_weights.append(point_weight)\n",
    "            batch_xij.append(cooc_mat[inp,lbl])\n",
    "        batch_weights = np.clip(batch_weights,-100,1)\n",
    "        batch_xij = np.asarray(batch_xij)        \n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss. Specifically we provide\n",
    "        # train_dataset/train_labels: training inputs and training labels\n",
    "        # weights_x: measures the importance of a data point with respect to how much those two words co-occur\n",
    "        # x_ij: co-occurence matrix value for the row and column denoted by the words in a datapoint\n",
    "        feed_dict = {train_dataset : batch_data.reshape(-1), train_labels : batch_labels.reshape(-1),\n",
    "                    weights_x:batch_weights,x_ij:batch_xij}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000          \n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            glove_loss.append(average_loss)\n",
    "            average_loss = 0        \n",
    "        if step % 10000 == 0:\n",
    "            print(time.strftime('%H:%M:%S',time.localtime()))\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print('valid_size%02d : '%i,log)            \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "tEnd = time.time()\n",
    "print('Cost %.2f seconds'%(tEnd-tStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
