{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09_VAE_fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "False\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets,layers,models,optimizers,Sequential,metrics,losses,utils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "print(sys.version)\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "(6000, 784) (6000,)\n",
      "(6000, 784) (6000,)\n"
     ]
    }
   ],
   "source": [
    "#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n",
    "class_names = [\"上衣\",\"褲子\",\"套衫\",\"連衣裙\",\"外套\",\"涼鞋\",\"襯衫\",\"運動鞋\",\"袋子\",\"踝靴\"]\n",
    "\n",
    "def data_normalize(train, test, scale=1):\n",
    "    (x_train,y_train) = train\n",
    "    (x_test,y_test) = test\n",
    "    train_nums = x_train.shape[0] // scale\n",
    "    test_nums = x_test.shape[0] // scale\n",
    "    x_train, y_train = x_train[:train_nums], y_train[:train_nums]\n",
    "    x_test, y_test = x_test[:train_nums], y_test[:train_nums]\n",
    "    x_train = x_train.astype(np.float32) / 255.\n",
    "    x_test = x_test.astype(np.float32) / 255.\n",
    "    \n",
    "    \n",
    "    x_train = tf.reshape(x_train, shape=(-1,28*28))\n",
    "    x_test = tf.reshape(x_test, shape=(-1,28*28))\n",
    "    #y_train = utils.to_categorical(y_train)#.astype(np.int32)\n",
    "    #y_test = utils.to_categorical(y_test)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    print(x_test.shape,y_test.shape)\n",
    "    \n",
    "    return (x_train,y_train),(x_test,y_test)\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "(x_train,y_train),(x_test,y_test) = data_normalize((x_train,y_train),(x_test,y_test),scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduce = 10\n",
    "\n",
    "class VAE(models.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "        #encoder\n",
    "        self.fc_layer_1 = layers.Dense(128)\n",
    "        self.fc_layer_2 = layers.Dense(dim_reduce)\n",
    "        self.fc_layer_3 = layers.Dense(dim_reduce)   \n",
    "        self.fc_layer_4 = layers.Dense(128)\n",
    "        self.fc_layer_5 = layers.Dense(784)\n",
    "\n",
    "    def model_encoder(self, x):\n",
    "        h = tf.nn.relu(self.fc_layer_1(x))\n",
    "        mean_fc = self.fc_layer_2(h)\n",
    "        var_fc = self.fc_layer_3(h)\n",
    "        return mean_fc,var_fc\n",
    "\n",
    "    def model_decoder(self, z):\n",
    "        out = tf.nn.relu(self.fc_layer_4(z))\n",
    "        out = self.fc_layer_5(out)\n",
    "        return out\n",
    "\n",
    "    def reparameter(self,mean_x,var_x):\n",
    "        eps = tf.random.normal(var_x.shape)\n",
    "        std = tf.exp(var_x)**0.5\n",
    "        z = mean_x + std*eps\n",
    "        return z\n",
    "  \n",
    "    def call(self, inputs, training=None):\n",
    "        mean_x,var_x = self.model_encoder(inputs)\n",
    "        z = self.reparameter(mean_x,var_x)\n",
    "        x = self.model_decoder(z)\n",
    "        return x,mean_x,var_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.build(input_shape=(4,784))\n",
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAIL\n",
    "# batch_size=128\n",
    "# epochs=6\n",
    "# optimizer = optimizers.Adam(lr=1e-3)\n",
    "# model.compile(loss=\"sigmoid_cross_entropy_with_logits\",optimizer=optimizer)\n",
    "# history = model.fit(x_train,x_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28) (28, 28)\n",
      "Model: \"vae_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim_reduce = 10\n",
    "batch_num = 128\n",
    "lr = 1e-3\n",
    "\n",
    "def feature_scale(x):\n",
    "    x = tf.cast(x,dtype=tf.float32)/255.\n",
    "    return x\n",
    "(x,y),(x_test,y_test) = datasets.fashion_mnist.load_data()\n",
    "data = tf.data.Dataset.from_tensor_slices(x)\n",
    "data = data.map(feature_scale).shuffle(10000).batch(batch_num)\n",
    "\n",
    "data_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "data_test = data_test.map(feature_scale).batch(batch_num)\n",
    "\n",
    "data_iter = iter(data)\n",
    "samples = next(data_iter)\n",
    "print(samples[0].shape,samples[1].shape)\n",
    "\n",
    "model = VAE()\n",
    "model.build(input_shape=(4,784))\n",
    "optimizer = optimizers.Adam(lr=lr)\n",
    "model.summary()\n",
    "\n",
    "optimizer = optimizers.Adam(lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 551.7018432617188 kl_div: 4.142721176147461\n",
      "0 100 loss: 313.9952087402344 kl_div: 14.881878852844238\n",
      "0 200 loss: 288.0704650878906 kl_div: 16.23518943786621\n",
      "0 300 loss: 261.48370361328125 kl_div: 15.778911590576172\n",
      "0 400 loss: 254.17156982421875 kl_div: 14.694847106933594\n",
      "1 0 loss: 279.21954345703125 kl_div: 15.134773254394531\n",
      "1 100 loss: 259.7195129394531 kl_div: 13.590936660766602\n",
      "1 200 loss: 265.6165466308594 kl_div: 14.831949234008789\n",
      "1 300 loss: 261.96588134765625 kl_div: 15.054460525512695\n",
      "1 400 loss: 257.9376525878906 kl_div: 13.888978958129883\n",
      "2 0 loss: 253.5751495361328 kl_div: 14.487897872924805\n",
      "2 100 loss: 251.00926208496094 kl_div: 14.010517120361328\n",
      "2 200 loss: 263.02459716796875 kl_div: 13.882869720458984\n",
      "2 300 loss: 258.3230895996094 kl_div: 14.516339302062988\n",
      "2 400 loss: 254.13148498535156 kl_div: 14.334625244140625\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for step,x in enumerate(data):\n",
    "        x = tf.reshape(x,[-1,784])\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits,mean_x,var_x = model(x)\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x,logits=logits)\n",
    "            loss = tf.reduce_sum(loss)/x.shape[0]\n",
    "            kl_div = -0.5*(var_x+1-mean_x**2-tf.exp(var_x))\n",
    "            kl_div = tf.reduce_sum(kl_div)/x.shape[0]\n",
    "\n",
    "            loss = loss + 1.*kl_div\n",
    "        grads = tape.gradient(loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "        if step %100==0:\n",
    "            print(i,step,'loss:',float(loss),'kl_div:',float(kl_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(imgs,names):\n",
    "    img_new = Image.new('L',(280,280))\n",
    "    index = 0\n",
    "    for i in range(0,280,80):\n",
    "        for j in range(0,280,80):\n",
    "            img = imgs[index]\n",
    "            img = Image.fromarray(img,mode='L')\n",
    "            img_new.paste(img,(i,j))\n",
    "            index+=1\n",
    "    img_new.save(names)\n",
    "\n",
    "x = next(iter(data_test))\n",
    "val_x = tf.reshape(x,[-1,784])\n",
    "logits,_,_ = model(val_x)\n",
    "x_hat = tf.sigmoid(logits)\n",
    "x_hat = tf.reshape(x_hat,[-1,28,28])\n",
    "x_hat = x_hat.numpy()*255\n",
    "x_hat = x_hat.astype(np.uint8)\n",
    "save_img(x_hat,'imgs/VAE_img_%d.png'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![efc](./imgs/VAE_img_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
